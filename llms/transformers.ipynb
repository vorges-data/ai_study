{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Model Implementation\n",
    "\n",
    "The script implements a simplified version of the Transformer model, which is a neural network architecture used primarily for natural language processing tasks.\n",
    "\n",
    "#### 1. **Softmax Function**\n",
    "Converts a vector of numbers into a probability distribution.\n",
    "\n",
    "- **Scaled Dot-Product Attention:** Calculates attention between queries (Q), keys (K), and values (V)\n",
    "- **Multi-Head Attention:** Allows the model to focus on different parts of the input simultaneously\n",
    "- **Feed-Forward Network:** Processes the outputs from the attention layer\n",
    "- **Encoder Layer:** Combines multi-head attention and the feed-forward network\n",
    "- **Transformer:** Combines multiple encoder layers and adds an embedding layer\n",
    "- **Data flow in the model:** The input is converted into embeddings. Each encoder layer processes the data:\n",
    "  - a. Multi-head attention calculates attention\n",
    "  - b. The feed-forward network processes the attention output\n",
    "\n",
    "#### 2. **Final Output**\n",
    "The final output represents the encoding of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 10)\n",
      "Output shape: (2, 10, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax function\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention\n",
    "    \"\"\"\n",
    "    # Calculate the dimension of the key vectors\n",
    "    d_k = K.shape[-1]\n",
    "\n",
    "    # Calculate the attention scores\n",
    "    attention_scores = np.einsum('...ij,...kj->...ik', Q, K) / np.sqrt(d_k)\n",
    "\n",
    "    # Apply softmax to the attention scores\n",
    "    attention_probs = softmax(attention_scores)\n",
    "\n",
    "    # Apply the attention probabilities to the value vectors\n",
    "    output = np.einsum('...ij,...jk->...ik', attention_probs, V)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        # Initialize the weights for the query, key, value, and output matrices+\n",
    "        self.WQ = np.random.randn(d_model, d_model)\n",
    "        self.WK = np.random.randn(d_model, d_model)\n",
    "        self.WV = np.random.randn(d_model, d_model)\n",
    "        self.WO = np.random.randn(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, depth)\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.transpose(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head attention\n",
    "        \"\"\"\n",
    "        batch_size = Q.shape[0]\n",
    "\n",
    "        # Project the queries, keys, and values\n",
    "        Q = np.matmul(Q, self.WQ)\n",
    "        K = np.matmul(K, self.WK)\n",
    "        V = np.matmul(V, self.WV)\n",
    "\n",
    "        # Split the heads\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        attention_output = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine the heads\n",
    "        attention_output = attention_output.transpose(0, 2, 1, 3).reshape(batch_size, -1, self.d_model)\n",
    "\n",
    "        # Project the output\n",
    "        output = np.matmul(attention_output, self.WO)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class FeedForward:\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        self.W1 = np.random.randn(d_model, d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for feed-forward network\n",
    "        \"\"\"\n",
    "        return np.matmul(np.maximum(np.matmul(x, self.W1), 0), self.W2)\n",
    "\n",
    "class EncoderLayer:\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for encoder layer\n",
    "        \"\"\"\n",
    "        # Apply multi-head attention\n",
    "        attention_output = self.mha.forward(x, x, x)\n",
    "\n",
    "        # Apply feed-forward network\n",
    "        ff_output = self.ff.forward(attention_output)\n",
    "\n",
    "        return ff_output\n",
    "\n",
    "# Transformer\n",
    "class Transformer:\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, vocab_size):\n",
    "        # Initialize the embedding layer\n",
    "        self.embedding = np.random.randn(vocab_size, d_model)\n",
    "\n",
    "        # Initialize the encoder layers\n",
    "        self.layers = [EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for transformer\n",
    "        \"\"\"\n",
    "        x = self.embedding[x]\n",
    "\n",
    "        # Apply the encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# Example usage\n",
    "vocab_size = 10000\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "d_ff = 128\n",
    "num_layers = 2\n",
    "\n",
    "# Create a transformer\n",
    "transformer = Transformer(num_layers, d_model, num_heads, d_ff, vocab_size)\n",
    "\n",
    "# Create a sample input\n",
    "input_seq = np.random.randint(0, vocab_size, size=(2, 10)) # 2 sequences of 10 tokens\n",
    "\n",
    "# Forward pass\n",
    "output = transformer.forward(input_seq)\n",
    "print(\"Input shape:\", input_seq.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Transformer Explanation\n",
    "\n",
    "## Overview\n",
    "This script implements a simplified version of the Transformer model, a neural network architecture widely used in natural language processing tasks.\n",
    "\n",
    "#### Key Components\n",
    "\n",
    "#### 1. Softmax Function\n",
    "* Converts a vector of numbers into a probability distribution.\n",
    "* Essential for attention weight calculation and output probability distributions.\n",
    "\n",
    "#### 2. Scaled Dot-Product Attention\n",
    "* Calculates attention between queries (Q), keys (K), and values (V).\n",
    "* Uses the formula: Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V\n",
    "* Allows the model to focus on relevant parts of the input sequence.\n",
    "\n",
    "#### 3. Multi-Head Attention\n",
    "* Runs multiple attention mechanisms in parallel.\n",
    "* Allows the model to focus on different parts of the input simultaneously.\n",
    "* Combines multiple \"attention heads\" to capture various types of relationships.\n",
    "\n",
    "#### 4. Feed-Forward Network\n",
    "* Processes the outputs from the attention layer.\n",
    "* Typically consists of two linear transformations with a ReLU activation in between.\n",
    "* Applies the same transformation to each position separately.\n",
    "\n",
    "#### 5. Encoder Layer\n",
    "* Combines multi-head attention and feed-forward network.\n",
    "* Includes residual connections and layer normalization (in full implementations).\n",
    "* Forms the basic building block of the Transformer encoder.\n",
    "\n",
    "#### 6. Transformer Model\n",
    "* Combines multiple encoder layers in sequence.\n",
    "* Adds an embedding layer to convert token indices to dense vectors.\n",
    "* Includes positional encoding to handle sequence order information.\n",
    "\n",
    "### Data Flow\n",
    "\n",
    "#### Input Processing\n",
    "1. **Token Embedding**: Input token indices are converted into dense vector embeddings.\n",
    "2. **Positional Encoding**: Position information is added to embeddings to maintain sequence order.\n",
    "\n",
    "#### Encoder Processing\n",
    "Each encoder layer processes the data through:\n",
    "1. **Multi-Head Attention**: Calculates attention weights and applies them to input representations.\n",
    "2. **Feed-Forward Processing**: Applies non-linear transformations to the attention outputs.\n",
    "3. **Residual Connections**: Adds input to output for better gradient flow (in full implementations).\n",
    "\n",
    "#### Output Generation\n",
    "The final output represents an encoded representation of the input sequence, where each position contains contextual information from the entire sequence.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "#### Step-by-Step Process\n",
    "1. **Input Preparation**: The model receives a sequence of token indices as input.\n",
    "2. **Embedding Conversion**: These indices are converted into dense embedding vectors.\n",
    "3. **Positional Information**: Position encodings are added to maintain order information.\n",
    "4. **Layer Processing**: Encoder layers sequentially process the embeddings:\n",
    "   - Apply self-attention to capture relationships between tokens\n",
    "   - Apply feed-forward transformations for non-linear processing\n",
    "5. **Output Generation**: The final output is an encoded representation of the input sequence.\n",
    "\n",
    "#### Key Mechanisms\n",
    "* **Self-Attention**: Allows each position to attend to all positions in the input sequence.\n",
    "* **Parallel Processing**: Unlike RNNs, Transformers can process all positions simultaneously.\n",
    "* **Context Awareness**: Each output position contains information from the entire input sequence.\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "#### Limitations of This Implementation\n",
    "* **Simplified Design**: This is a basic model for educational purposes.\n",
    "* **Missing Features**: Does not include advanced features such as:\n",
    "  - Attention masking for decoder applications\n",
    "  - Layer normalization for training stability\n",
    "  - Dropout for regularization\n",
    "  - Proper weight initialization\n",
    "  - Training procedures and optimization\n",
    "\n",
    "#### Educational Purpose\n",
    "* **Learning Tool**: Serves as an introduction to fundamental Transformer concepts.\n",
    "* **Foundation**: Provides a base understanding before exploring more complex implementations.\n",
    "* **Conceptual Understanding**: Focuses on core mechanisms rather than production-ready features.\n",
    "\n",
    "#### Real-World Applications\n",
    "Full Transformer implementations are used in:\n",
    "* **Language Models**: GPT, BERT, T5\n",
    "* **Machine Translation**: Google Translate improvements\n",
    "* **Text Summarization**: Automatic document summarization\n",
    "* **Question Answering**: Conversational AI systems\n",
    "* **Code Generation**: Programming assistance tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (LLMS)",
   "language": "python",
   "name": "llms-py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
