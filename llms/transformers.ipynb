{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Model Implementation\n",
    "\n",
    "The script implements a simplified version of the Transformer model, which is a neural network architecture used primarily for natural language processing tasks.\n",
    "\n",
    "#### 1. **Softmax Function**\n",
    "Converts a vector of numbers into a probability distribution.\n",
    "\n",
    "- **Scaled Dot-Product Attention:** Calculates attention between queries (Q), keys (K), and values (V)\n",
    "- **Multi-Head Attention:** Allows the model to focus on different parts of the input simultaneously\n",
    "- **Feed-Forward Network:** Processes the outputs from the attention layer\n",
    "- **Encoder Layer:** Combines multi-head attention and the feed-forward network\n",
    "- **Transformer:** Combines multiple encoder layers and adds an embedding layer\n",
    "- **Data flow in the model:** The input is converted into embeddings. Each encoder layer processes the data:\n",
    "  - a. Multi-head attention calculates attention\n",
    "  - b. The feed-forward network processes the attention output\n",
    "\n",
    "#### 2. **Final Output**\n",
    "The final output represents the encoding of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax function\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention\n",
    "    \"\"\"\n",
    "    # Calculate the dimension of the key vectors\n",
    "    d_k = K.shape[-1]\n",
    "\n",
    "    # Calculate the attention scores\n",
    "    attention_scores = np.einsum('...ij,...kj->...ik', Q, K) / np.sqrt(d_k)\n",
    "\n",
    "    # Apply softmax to the attention scores\n",
    "    attention_probs = softmax(attention_scores)\n",
    "\n",
    "    # Apply the attention probabilities to the value vectors\n",
    "    output = np.einsum('...ij,...jk->...ik', attention_probs, V)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        # Initialize the weights for the query, key, value, and output matrices+\n",
    "        self.WQ = np.random.randn(d_model, d_model)\n",
    "        self.WK = np.random.randn(d_model, d_model)\n",
    "        self.WV = np.random.randn(d_model, d_model)\n",
    "        self.WO = np.random.randn(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, depth)\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.transpose(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head attention\n",
    "        \"\"\"\n",
    "        batch_size = Q.shape[0]\n",
    "\n",
    "        # Project the queries, keys, and values\n",
    "        Q = np.matmul(Q, self.WQ)\n",
    "        K = np.matmul(K, self.WK)\n",
    "        V = np.matmul(V, self.WV)\n",
    "\n",
    "        # Split the heads\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        attention_output = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine the heads\n",
    "        attention_output = attention_output.transpose(0, 2, 1, 3).reshape(batch_size, -1, self.d_model)\n",
    "\n",
    "        # Project the output\n",
    "        output = np.matmul(attention_output, self.WO)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class FeedForward:\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        self.W1 = np.random.randn(d_model, d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for feed-forward network\n",
    "        \"\"\"\n",
    "        return np.matmul(np.maximum(np.matmul(x, self.W1), 0), self.W2)\n",
    "\n",
    "class EncoderLayer:\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for encoder layer\n",
    "        \"\"\"\n",
    "        # Apply multi-head attention\n",
    "        attention_output = self.mha.forward(x, x, x)\n",
    "\n",
    "        # Apply feed-forward network\n",
    "        ff_output = self.ff.forward(attention_output)\n",
    "\n",
    "        return ff_output\n",
    "\n",
    "# Transformer\n",
    "class Transformer:\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, vocab_size):\n",
    "        # Initialize the embedding layer\n",
    "        self.embedding = np.random.randn(vocab_size, d_model)\n",
    "\n",
    "        # Initialize the encoder layers\n",
    "        self.layers = [EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for transformer\n",
    "        \"\"\"\n",
    "        x = self.embedding[x]\n",
    "\n",
    "        # Apply the encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# Example usage\n",
    "vocab_size = 10000\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "d_ff = 128\n",
    "num_layers = 2\n",
    "\n",
    "# Create a transformer\n",
    "transformer = Transformer(num_layers, d_model, num_heads, d_ff, vocab_size)\n",
    "\n",
    "# Create a sample input\n",
    "input_seq = np.random.randint(0, vocab_size, size=(2, 10)) # 2 sequences of 10 tokens\n",
    "\n",
    "# Forward pass\n",
    "output = transformer.forward(input_seq)\n",
    "print(\"Input shape:\", input_seq.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (LLMS)",
   "language": "python",
   "name": "llms-py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
