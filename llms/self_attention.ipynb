{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original phrase:  The quick brown fox jumps over the lazy dog\n",
      "\n",
      "Weighted of Attention:\n",
      "\n",
      "For the word: 'The':\n",
      "Attention for 'The': 0.1131\n",
      "Attention for 'quick': 0.0741\n",
      "Attention for 'brown': 0.0906\n",
      "Attention for 'fox': 0.1132\n",
      "Attention for 'jumps': 0.0776\n",
      "Attention for 'over': 0.0753\n",
      "Attention for 'the': 0.1077\n",
      "Attention for 'lazy': 0.0616\n",
      "Attention for 'dog': 0.0959\n",
      "\n",
      "For the word: 'quick':\n",
      "Attention for 'The': 0.1678\n",
      "Attention for 'quick': 0.2321\n",
      "Attention for 'brown': 0.1320\n",
      "Attention for 'fox': 0.1498\n",
      "Attention for 'jumps': 0.1795\n",
      "Attention for 'over': 0.1958\n",
      "Attention for 'the': 0.1433\n",
      "Attention for 'lazy': 0.2086\n",
      "Attention for 'dog': 0.1957\n",
      "\n",
      "For the word: 'brown':\n",
      "Attention for 'The': 0.0595\n",
      "Attention for 'quick': 0.0383\n",
      "Attention for 'brown': 0.0988\n",
      "Attention for 'fox': 0.0714\n",
      "Attention for 'jumps': 0.0602\n",
      "Attention for 'over': 0.0518\n",
      "Attention for 'the': 0.0820\n",
      "Attention for 'lazy': 0.0490\n",
      "Attention for 'dog': 0.0482\n",
      "\n",
      "For the word: 'fox':\n",
      "Attention for 'The': 0.0933\n",
      "Attention for 'quick': 0.0545\n",
      "Attention for 'brown': 0.0896\n",
      "Attention for 'fox': 0.0997\n",
      "Attention for 'jumps': 0.0675\n",
      "Attention for 'over': 0.0621\n",
      "Attention for 'the': 0.0966\n",
      "Attention for 'lazy': 0.0485\n",
      "Attention for 'dog': 0.0755\n",
      "\n",
      "For the word: 'jumps':\n",
      "Attention for 'The': 0.0978\n",
      "Attention for 'quick': 0.0999\n",
      "Attention for 'brown': 0.1156\n",
      "Attention for 'fox': 0.1033\n",
      "Attention for 'jumps': 0.1733\n",
      "Attention for 'over': 0.0983\n",
      "Attention for 'the': 0.0975\n",
      "Attention for 'lazy': 0.0863\n",
      "Attention for 'dog': 0.1006\n",
      "\n",
      "For the word: 'over':\n",
      "Attention for 'The': 0.1438\n",
      "Attention for 'quick': 0.1653\n",
      "Attention for 'brown': 0.1508\n",
      "Attention for 'fox': 0.1439\n",
      "Attention for 'jumps': 0.1490\n",
      "Attention for 'over': 0.1932\n",
      "Attention for 'the': 0.1283\n",
      "Attention for 'lazy': 0.1612\n",
      "Attention for 'dog': 0.1638\n",
      "\n",
      "For the word: 'the':\n",
      "Attention for 'The': 0.0664\n",
      "Attention for 'quick': 0.0391\n",
      "Attention for 'brown': 0.0770\n",
      "Attention for 'fox': 0.0723\n",
      "Attention for 'jumps': 0.0478\n",
      "Attention for 'over': 0.0414\n",
      "Attention for 'the': 0.0890\n",
      "Attention for 'lazy': 0.0460\n",
      "Attention for 'dog': 0.0492\n",
      "\n",
      "For the word: 'lazy':\n",
      "Attention for 'The': 0.1077\n",
      "Attention for 'quick': 0.1611\n",
      "Attention for 'brown': 0.1305\n",
      "Attention for 'fox': 0.1028\n",
      "Attention for 'jumps': 0.1197\n",
      "Attention for 'over': 0.1476\n",
      "Attention for 'the': 0.1305\n",
      "Attention for 'lazy': 0.2318\n",
      "Attention for 'dog': 0.1192\n",
      "\n",
      "For the word: 'dog':\n",
      "Attention for 'The': 0.1505\n",
      "Attention for 'quick': 0.1357\n",
      "Attention for 'brown': 0.1152\n",
      "Attention for 'fox': 0.1437\n",
      "Attention for 'jumps': 0.1253\n",
      "Attention for 'over': 0.1346\n",
      "Attention for 'the': 0.1251\n",
      "Attention for 'lazy': 0.1070\n",
      "Attention for 'dog': 0.1519\n",
      "\n",
      "Explanation:\n",
      "The has the highest attention weight with fox\n",
      "quick has the highest attention weight with quick\n",
      "brown has the highest attention weight with brown\n",
      "fox has the highest attention weight with fox\n",
      "jumps has the highest attention weight with jumps\n",
      "over has the highest attention weight with over\n",
      "the has the highest attention weight with the\n",
      "lazy has the highest attention weight with lazy\n",
      "dog has the highest attention weight with dog\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example phrase\n",
    "phrase = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Function to make a word embedding\n",
    "def make_word_embedding(word):\n",
    "    return np.random.rand(4)\n",
    "\n",
    "# Softmax function\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# Making the word embeddings\n",
    "word = phrase.split()\n",
    "word_embeddings = [make_word_embedding(w) for w in word]\n",
    "\n",
    "# Simple self-attention function\n",
    "def self_attention(word_embeddings):\n",
    "    # Making matrix of word embeddings\n",
    "    Q = np.array(word_embeddings)\n",
    "    K = np.array(word_embeddings)\n",
    "    V = np.array(word_embeddings)\n",
    "\n",
    "    # Calculating the attention scores\n",
    "    scores = np.dot(Q, K.T)\n",
    "\n",
    "    # Applying the softmax function to the scores\n",
    "    attention_weights = softmax(scores)\n",
    "\n",
    "    # Calculating the weighted sum of the word embeddings\n",
    "    weighted_values = np.dot(attention_weights, V)\n",
    "\n",
    "    return attention_weights, weighted_values\n",
    "\n",
    "# Apply the self-attention function to the word embeddings\n",
    "attention_weights, weighted_values = self_attention(word_embeddings)\n",
    "\n",
    "# Print the attention weights and weighted values\n",
    "print(\"Original phrase: \", phrase)\n",
    "print(\"\\nWeighted of Attention:\")\n",
    "for i, w in enumerate(word):\n",
    "    print(f\"\\nFor the word: '{w}':\")\n",
    "    for j, w2 in enumerate(word):\n",
    "        print(f\"Attention for '{w2}': {attention_weights[i][j]:.4f}\")\n",
    "\n",
    "print(\"\\nExplanation:\")\n",
    "for i, w in enumerate(word):\n",
    "    max_attention_weight = np.argmax(attention_weights[i])\n",
    "    print(f\"{w} has the highest attention weight with {word[max_attention_weight]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Example of Self-Attention\n",
    "\n",
    "This notebook demonstrates a simplified example of how the **self-attention** mechanism works using a basic sentence.\n",
    "\n",
    "### Overview\n",
    "\n",
    "- We use the sentence “The quick brown fox jumps over the lazy dog” as an example.\n",
    "- Simple random embeddings are generated for each word.\n",
    "- A simplified version of the self-attention mechanism is implemented.\n",
    "- We analyze how each word relates to the others in the sentence based on attention weights.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "**Embedding Creation**\n",
    "\n",
    "- The `make_word_embedding` function generates random vectors to represent words.\n",
    "- In real models, these embeddings are learned during training and capture semantic meaning.\n",
    "\n",
    "**Softmax Function**\n",
    "\n",
    "- Converts attention scores into probabilities, allowing the model to assign different levels of importance to each word in context.\n",
    "\n",
    "**Simplified Self-Attention**\n",
    "\n",
    "- The `self_attention` function simulates the self-attention process:\n",
    "  - Creates Q (query), K (key), and V (value) matrices from the same word embeddings.\n",
    "  - Computes attention scores by taking the dot product between Q and K.\n",
    "  - Applies softmax to the scores to obtain attention weights.\n",
    "  - Computes weighted values by multiplying attention weights with the V matrix.\n",
    "\n",
    "### Process\n",
    "\n",
    "1. The sentence is split into words.\n",
    "2. Each word is transformed into a random embedding vector.\n",
    "3. The self-attention function is applied to compute how each word \"attends\" to the others.\n",
    "4. The output displays attention weights and which word received the most attention from each one.\n",
    "\n",
    "### Results\n",
    "\n",
    "- The code prints the attention weight matrix between each pair of words.\n",
    "- It also provides a simple interpretation of which word receives the most attention from each word.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- This is a highly simplified, illustrative example for educational purposes.\n",
    "- In real Transformer models, the attention mechanism is more complex, including:\n",
    "  - Separate linear projections for Q, K, and V.\n",
    "  - Scaling of attention scores.\n",
    "  - Multi-head attention.\n",
    "  - Additional layers and learned parameters during training.\n",
    "\n",
    "However, this example helps visualize how context is considered in natural language processing through self-attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original phrase:  Vorges Data is a personal blog about data science and machine learning.\n",
      "\n",
      "Weighted of Attention:\n",
      "\n",
      "For the word: 'Vorges':\n",
      "Attention for 'Vorges': 0.1248\n",
      "Attention for 'science': 0.0734\n",
      "Attention for 'blog': 0.0723\n",
      "\n",
      "For the word: 'Data':\n",
      "Attention for 'about': 0.1097\n",
      "Attention for 'Data': 0.1032\n",
      "Attention for 'is': 0.0911\n",
      "\n",
      "For the word: 'is':\n",
      "Attention for 'is': 0.0710\n",
      "Attention for 'personal': 0.0695\n",
      "Attention for 'about': 0.0635\n",
      "\n",
      "For the word: 'a':\n",
      "Attention for 'a': 0.1309\n",
      "Attention for 'machine': 0.1191\n",
      "Attention for 'science': 0.1128\n",
      "\n",
      "For the word: 'personal':\n",
      "Attention for 'personal': 0.0705\n",
      "Attention for 'is': 0.0629\n",
      "Attention for 'science': 0.0555\n",
      "\n",
      "For the word: 'blog':\n",
      "Attention for 'blog': 0.1416\n",
      "Attention for 'data': 0.0978\n",
      "Attention for 'Vorges': 0.0870\n",
      "\n",
      "For the word: 'about':\n",
      "Attention for 'about': 0.1224\n",
      "Attention for 'Data': 0.1098\n",
      "Attention for 'is': 0.0914\n",
      "\n",
      "For the word: 'data':\n",
      "Attention for 'data': 0.1409\n",
      "Attention for 'blog': 0.1319\n",
      "Attention for 'learning.': 0.1147\n",
      "\n",
      "For the word: 'science':\n",
      "Attention for 'science': 0.0785\n",
      "Attention for 'is': 0.0760\n",
      "Attention for 'Vorges': 0.0758\n",
      "\n",
      "For the word: 'and':\n",
      "Attention for 'and': 0.1529\n",
      "Attention for 'Vorges': 0.1297\n",
      "Attention for 'learning.': 0.1242\n",
      "\n",
      "For the word: 'machine':\n",
      "Attention for 'a': 0.1125\n",
      "Attention for 'machine': 0.1103\n",
      "Attention for 'science': 0.1012\n",
      "\n",
      "For the word: 'learning.':\n",
      "Attention for 'learning.': 0.1842\n",
      "Attention for 'data': 0.1601\n",
      "Attention for 'about': 0.1499\n",
      "\n",
      "Explanation:\n",
      "Vorges has the highest attention weight with Vorges\n",
      "Data has the highest attention weight with about\n",
      "is has the highest attention weight with is\n",
      "a has the highest attention weight with a\n",
      "personal has the highest attention weight with personal\n",
      "blog has the highest attention weight with blog\n",
      "about has the highest attention weight with about\n",
      "data has the highest attention weight with data\n",
      "science has the highest attention weight with science\n",
      "and has the highest attention weight with and\n",
      "machine has the highest attention weight with a\n",
      "learning. has the highest attention weight with learning.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example phrase\n",
    "phrase = \"Vorges Data is a personal blog about data science and machine learning.\"\n",
    "\n",
    "# Function to make a simple embedding\n",
    "def make_embedding(word):\n",
    "    return np.random.rand(4)\n",
    "\n",
    "# Function to make a simple softmax\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# Making embeddings for the words\n",
    "words = phrase.split()\n",
    "word_embeddings = [make_embedding(w) for w in words]\n",
    "\n",
    "# Making the attention weights\n",
    "def attention_weights(word_embeddings):\n",
    "    # Making the query, key and value matrices\n",
    "    Q = np.array(word_embeddings)\n",
    "    K = np.array(word_embeddings)\n",
    "    V = np.array(word_embeddings)\n",
    "\n",
    "    # Calculating the attention scores\n",
    "    scores = np.dot(Q, K.T)\n",
    "\n",
    "    # Applying the softmax function to the scores\n",
    "    attention_weights = softmax(scores)\n",
    "\n",
    "    # Calculating the weighted sum of the word embeddings\n",
    "    weighted_values = np.dot(attention_weights, V)\n",
    "\n",
    "    return attention_weights, weighted_values\n",
    "\n",
    "# Apply self-attention to the word embeddings\n",
    "attention_weights, weighted_values = attention_weights(word_embeddings)\n",
    "\n",
    "# Print the attention weights and weighted values\n",
    "print(\"Original phrase: \", phrase)\n",
    "print(\"\\nWeighted of Attention:\")\n",
    "for i, w in enumerate(words):\n",
    "    print(f\"\\nFor the word: '{w}':\")\n",
    "    # Print only 3 words with the highest attention weights\n",
    "    top_indices = sorted(range(len(words)), key=lambda j: attention_weights[i][j], reverse=True)[:3]\n",
    "    for j in top_indices:\n",
    "        print(f\"Attention for '{words[j]}': {attention_weights[i][j]:.4f}\")\n",
    "\n",
    "print(\"\\nExplanation:\")\n",
    "for i, w in enumerate(words):\n",
    "    max_attention_weight = np.argmax(attention_weights[i])\n",
    "    print(f\"{w} has the highest attention weight with {words[max_attention_weight]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Example of Simplified Self-Attention\n",
    "\n",
    "This notebook explores two simplified implementations of the **self-attention** mechanism using different sentences. Both examples aim to illustrate how words in a sentence \"attend to\" each other based on their vector representations (embeddings).\n",
    "\n",
    "---\n",
    "\n",
    "### Example 1 — Short Sentence\n",
    "\n",
    "**Sentence:**\n",
    "\"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "**Overview:**\n",
    "- Consists of 9 words.\n",
    "- All words are treated equally and embedded into random vectors.\n",
    "- Attention weights are calculated for every word pair.\n",
    "- The result shows attention for all words per row in the matrix.\n",
    "\n",
    "**Output Style:**\n",
    "- Displays full attention weights (9x9 matrix).\n",
    "- For each word, shows attention with all others in the sentence.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 2 — Longer and Richer Sentence\n",
    "\n",
    "**Sentence:**\n",
    "\"Vorges Data is a personal blog about data science and machine learning.\"\n",
    "\n",
    "**Overview:**\n",
    "- Consists of 11 words, including domain-specific terms.\n",
    "- Same attention mechanism is used: embeddings, dot product for attention scores, softmax normalization, and weighted value computation.\n",
    "- Introduces named entities like \"Vorges Data\" and multi-word concepts such as \"machine learning\".\n",
    "\n",
    "**Output Style:**\n",
    "- To improve readability, the output is limited to only the **top 3 most attended words** for each word.\n",
    "- This makes the results easier to interpret with a larger attention matrix (11x11).\n",
    "\n",
    "---\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Aspect                        | Example 1                            | Example 2                                 |\n",
    "|------------------------------|--------------------------------------|-------------------------------------------|\n",
    "| Sentence length              | 9 words                              | 11 words                                  |\n",
    "| Output detail                | Full matrix for all word pairs       | Top 3 attention weights per word          |\n",
    "| Named entities               | None                                  | \"Vorges Data\", \"machine learning\"         |\n",
    "| Context complexity           | Simple sentence with generic words   | Richer context with technical vocabulary  |\n",
    "| Readability adjustment       | None                                  | Output trimmed for clarity                |\n",
    "\n",
    "---\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "- These examples are **simplified** for educational purposes.\n",
    "- In real-world models (e.g., Transformers):\n",
    "  - Q, K, and V are derived through separate learned linear projections.\n",
    "  - Attention is computed across multiple **heads** and **layers**.\n",
    "  - Embeddings are pre-trained and semantically rich.\n",
    "  - Attention is often scaled and masked depending on the task (e.g., causal masking for language generation).\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "These two examples demonstrate how the self-attention mechanism adapts to sentences of different lengths and complexities. While the underlying logic remains the same, longer or more semantically rich sentences require better visualization strategies (like limiting output) to make interpretation easier. This reflects a core strength of attention mechanisms: flexibility in handling variable-length input and contextual relevance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (LLMS)",
   "language": "python",
   "name": "llms-py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
